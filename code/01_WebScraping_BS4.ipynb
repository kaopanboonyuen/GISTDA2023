{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_3C2-BbEQ67J"
      },
      "source": [
        "# A Simple Introduction to Web Scraping with Beautiful Soup\n",
        "\n",
        "![](https://github.com/kaopanboonyuen/GISTDA2023/raw/main/img/gistda_day1.png)\n",
        "\n",
        "\n",
        "Credit: \n",
        "\n",
        "[1] https://realpython.com/beautiful-soup-web-scraper-python/\n",
        "\n",
        "[2] https://www.analyticsvidhya.com/blog/2021/08/a-simple-introduction-to-web-scraping-with-beautiful-soup/\n",
        "\n",
        "[3] https://www.scrapingbee.com/blog/python-web-scraping-beautiful-soup/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eufi7kMfMYmY"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup \n",
        "import requests\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WUADewBeevMG"
      },
      "source": [
        "Beautiful Soup is a library useful to extract data from HTML and XML files. A sort of parse tree is built for the parsed page. Indeed, an HTML document is composed of a tree of tags. I will show an example of HTML code to make you grasp this concept.\n",
        "\n",
        "\n",
        "<!-- <!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "<title>Tutorial of Web scraping</title>\n",
        "</head>\n",
        "<body>\n",
        "<h1>1. Import libraries</h1>\n",
        "<p>Let's import: </p>\n",
        "</body>\n",
        "</html> -->\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*iOWLHDOtqxgngIOj9N3Hzw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLuT23oQerEi",
        "outputId": "2a14a024-91b9-4636-8b49-bb4b138120f7"
      },
      "outputs": [],
      "source": [
        "url = 'https://en.wikipedia.org/wiki/Big_data'\n",
        "req = requests.get(url)\n",
        "print(req)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brudDXWMfGGQ",
        "outputId": "e3541d5c-2fb4-443a-d839-16e7328d8475"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(req.text,\"html.parser\")\n",
        "print(type(soup))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxeGYkVwk87m",
        "outputId": "c8ccee0b-bdba-48c0-b429-25cbf53d968b"
      },
      "outputs": [],
      "source": [
        "print(soup.prettify()[:100])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JFs_oWEGlqBw"
      },
      "source": [
        "## Beautiful Soup DOM Tree\n",
        "The structure of Beautiful Soup bases on the concept of DOM, which is used in all web browsers.  DOM is a tree of all elements in the webpage.  Each element node consists of:\n",
        "- tag\n",
        "- innerHTML/outerHTML\n",
        "- id\n",
        "- attributes\n",
        "- parent and children\n",
        "\n",
        "Note: DOM = Document Object Model \n",
        "\n",
        "### Traversing simple HTML's DOM Tree\n",
        "\n",
        "In our example, the structure is as followed:\n",
        "\n",
        "```\n",
        "html\n",
        "+-- head\n",
        "|   +-- title\n",
        "|   +-- meta\n",
        "|   +-- meta\n",
        "|   +-- style\n",
        "+-- body\n",
        "    +-- div\n",
        "    |   +-- h1\n",
        "    |   +-- p\n",
        "    |       +-- b\n",
        "    +-- div\n",
        "    |   +-- a\n",
        "    |   +-- a\n",
        "    |   +-- a\n",
        "    |   +-- a\n",
        "    +-- div\n",
        "    |   +--div\n",
        "    |   |   +-- h2\n",
        "    |   |   +-- h5\n",
        "    |   |   +-- ...\n",
        "    |   +--div\n",
        "    |       +-- h2\n",
        "    |       +-- h5\n",
        "    |       +-- ...\n",
        "    +-- div\n",
        "        +-- h2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj9NOzb_k8-c",
        "outputId": "94fb38c7-f77e-47ec-8374-cc8453cfb6ba"
      },
      "outputs": [],
      "source": [
        "# title is a tag of one of the element node in the example.\n",
        "# we can refer to the node by using the tag name\n",
        "type(soup.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urWWh4neltzo"
      },
      "outputs": [],
      "source": [
        "soup.head.style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-TcKs_KNlvMH",
        "outputId": "2d2be527-0a8a-4511-dd01-dc90b3e417f9"
      },
      "outputs": [],
      "source": [
        "# we can get tag of a node with 'name'\n",
        "soup.title.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SIHrhP6klwgr",
        "outputId": "9d13f92c-6c47-4ce0-9ed6-15604dbb15c8"
      },
      "outputs": [],
      "source": [
        "# we can get outerHTML by converting node to string\n",
        "str(soup.title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YmZt-kxElyvZ",
        "outputId": "0d3762f8-608c-4a01-810c-30afd6c8ca77"
      },
      "outputs": [],
      "source": [
        "# we can get innerHTML with 'string'\n",
        "soup.title.string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmNL1IU0l3MK"
      },
      "outputs": [],
      "source": [
        "# we can get id with 'id' (it is empty in this example)\n",
        "soup.title.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DpJImW4rl435",
        "outputId": "05b1a54c-59ed-46d5-b212-301f6a0ffc37"
      },
      "outputs": [],
      "source": [
        "# getting the parent node with 'parent'\n",
        "soup.title.parent.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTqWDUzql60B",
        "outputId": "ea713015-5725-4739-dce3-8a69f3e86f1c"
      },
      "outputs": [],
      "source": [
        "# referring to children\n",
        "soup.title.children"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VurKAsXnVLG7"
      },
      "source": [
        "# Intro to Beautiful Soup: Build a Web Scraper With Python\n",
        "\n",
        "![](https://realpython.com/cdn-cgi/image/width=960,format=auto/https://files.realpython.com/media/Build-a-Web-Scraper-With-Requests-and-Beautiful-Soup_Watermarked.37918fb3906c.jpg)\n",
        "\n",
        "Credit: https://realpython.com/beautiful-soup-web-scraper-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTi1J0AQVKP8",
        "outputId": "1824b0c2-7e95-44ca-9a65-6e080e6f5f42"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = \"https://realpython.github.io/fake-jobs/\"\n",
        "page = requests.get(URL)\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "results = soup.find(id=\"ResultsContainer\")\n",
        "\n",
        "job_elements = results.find_all(\"div\", class_=\"card-content\")\n",
        "\n",
        "for job_element in job_elements:\n",
        "    print(job_element, end=\"\\n\"*2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VK6K_zCVWj8",
        "outputId": "abb038e3-0822-4f60-955e-7796cf575416"
      },
      "outputs": [],
      "source": [
        "for job_element in job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "    print(title_element)\n",
        "    print(company_element)\n",
        "    print(location_element)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwMRuMC5VWhW",
        "outputId": "493f345e-45bc-4c9e-db4e-4e31c21b6053"
      },
      "outputs": [],
      "source": [
        "for job_element in job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "    print(title_element.text.strip())\n",
        "    print(company_element.text.strip())\n",
        "    print(location_element.text.strip())\n",
        "    print()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G9m7MBM5Cqyz"
      },
      "source": [
        "# Wikipedia Page Data Extraction\n",
        "\n",
        "In this tutorial, we will learn how to extract a static page and convert it into useful information.\n",
        "\n",
        "We first get a wikipeidia page using requests.\n",
        "\n",
        "![](https://www.techhub.in.th/wp-content/uploads/2013/12/wikipedia-logo.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO0FMboVB-YT"
      },
      "outputs": [],
      "source": [
        "bigdata = requests.get('https://en.wikipedia.org/wiki/Big_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vJfUxFsB-ao",
        "outputId": "86625d94-d73b-4b70-a6ca-3ef4351d215a"
      },
      "outputs": [],
      "source": [
        "len(bigdata.text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2dRLrgFXCCg4"
      },
      "source": [
        "## Parsing a wikipedia page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCUgKUrxB-cu"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(bigdata.text, \"lxml\")\n",
        "#print(soup.prettify())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ki_DXKKUCGm2",
        "outputId": "ff3a7a0a-a3e0-4689-ba04-e3e98b22988b"
      },
      "outputs": [],
      "source": [
        "soup.title.string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fih4EHNhCGj-"
      },
      "outputs": [],
      "source": [
        "# soup.find_all('a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqXjXjPHCGhI",
        "outputId": "f600c281-536b-4f1a-d186-9e30ce755f90"
      },
      "outputs": [],
      "source": [
        "for link in soup.find_all('a', limit=15):\n",
        "    print('{} : {}'.format(link.get('class'), link.get('href')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6HnU8zoCGdv"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r'/wiki/(.*)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwKf07o2CO1L",
        "outputId": "74e3dcf4-7423-466c-ec6f-9ad1ce06ffd5"
      },
      "outputs": [],
      "source": [
        "for link in soup.find_all('a', {'class': None}, limit=20):\n",
        "    href = link.get('href')\n",
        "    if href is not None:\n",
        "        match = re.match(pattern, href)\n",
        "        if match:\n",
        "            print(href)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMqR4gimCOp_",
        "outputId": "824dc906-e6ce-4d60-ee9f-27e4d4afe3ef"
      },
      "outputs": [],
      "source": [
        "a_list = soup.select('div.div-col ul a')\n",
        "a_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVLr7YyNCZBP",
        "outputId": "0320b5db-8d0c-494b-b1e4-a2fdaac17d9c"
      },
      "outputs": [],
      "source": [
        "for e in a_list:\n",
        "    print(e['href'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FTYD7UOCY-G"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for e in a_list:\n",
        "    data.append({ 'keyword' : e.string, 'link' : e['href'] })\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "vq57_FdMCY33",
        "outputId": "cd56a7a5-1565-4f4f-8725-4c56dcc06709"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U5xdWHt-DGF4"
      },
      "source": [
        "# REST API Data Extraction\n",
        "\n",
        "![](https://raw.githubusercontent.com/Codecademy/articles/0b631b51723fbb3cc652ef5f009082aa71916e63/images/rest_api.svg)\n",
        "\n",
        "Gathering data from a REST API is quite typical.  Most Single-Page-Application (SPA) and AJAX dynamic pages rely on REST APIs.  In addition, most vendor-specific APIs such as Facebook, Twitter, etc., base on REST.\n",
        "\n",
        "The most important step of extracting data via REST API is to identify the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7m7gRGrDFYz"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXII1UBiDIUC"
      },
      "outputs": [],
      "source": [
        "api_url = 'http://api.settrade.com/api/market/SET/info'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "zgcQJUAcDIQ7",
        "outputId": "5901e274-7418-4ac8-adca-34543eed3288"
      },
      "outputs": [],
      "source": [
        "data_info = requests.get(api_url)\n",
        "data_info.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1mVYogPDIL-",
        "outputId": "742fe571-b0af-4e79-f8e5-c5638c3d61bc"
      },
      "outputs": [],
      "source": [
        "set_info = json.loads(data_info.text)\n",
        "pprint.pprint(set_info['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZISp14NDIFv",
        "outputId": "d3896b32-fcf3-46c9-e658-00b8f0ec8fba"
      },
      "outputs": [],
      "source": [
        "market = set_info['index'][0]\n",
        "print(market['market'], market['last'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNcx6CrFDRGS",
        "outputId": "ced34bec-1160-458a-8856-503cb4df711b"
      },
      "outputs": [],
      "source": [
        "for ind in set_info['index']:\n",
        "    print(ind['index_name'], ind['last'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_fHiCERJGCMF"
      },
      "source": [
        "## Data Table Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfOh0NFQDf9F"
      },
      "outputs": [],
      "source": [
        "# Send a GET request to the website\n",
        "response = requests.get(\"https://www.w3schools.com/html/html_tables.asp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZzq9eu0F8bD"
      },
      "outputs": [],
      "source": [
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Find the table you want to scrape\n",
        "table = soup.find(\"table\", {\"id\": \"customers\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P93PB92F8Y9"
      },
      "outputs": [],
      "source": [
        "# Extract the table headers\n",
        "headers = []\n",
        "for th in table.find_all(\"th\"):\n",
        "    headers.append(th.text.strip())\n",
        "\n",
        "# Extract the table rows and cells\n",
        "rows = []\n",
        "for tr in table.find_all(\"tr\"):\n",
        "    cells = []\n",
        "    for td in tr.find_all(\"td\"):\n",
        "        cells.append(td.text.strip())\n",
        "    if cells:\n",
        "        rows.append(cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "rRPEoQHpF8WK",
        "outputId": "b260dff9-388a-4a3f-f5dd-e041ac59fc42"
      },
      "outputs": [],
      "source": [
        "# Store the table data in a Pandas DataFrame\n",
        "df = pd.DataFrame(rows, columns=headers)\n",
        "df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F0EAO797N4RA"
      },
      "source": [
        "# Real Cases: BBC news homepage\n",
        "\n",
        "![](https://d.newsweek.com/en/full/881613/33-bbc-breaking-news.jpg?w=466&h=311&f=0717db3d760d0f8559be00d641c9f167)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rNiDYFwGvVp",
        "outputId": "d6091db7-e026-4337-ad23-d0d0c4b42eeb"
      },
      "outputs": [],
      "source": [
        "# make a GET request to the BBC news homepage\n",
        "response = requests.get('https://www.bbc.com/news')\n",
        "\n",
        "# create a BeautifulSoup object from the response content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# find all the main news headlines and their URLs\n",
        "main_headlines = soup.find_all('a', class_='gs-c-promo-heading')\n",
        "\n",
        "# iterate through the headlines and print their text and URLs\n",
        "for headline in main_headlines:\n",
        "    print('headline:',headline.text.strip())\n",
        "    print('href:',headline['href'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0wnn7qAKNgW",
        "outputId": "2163e892-05b7-4604-cfd6-f78c3e615b19"
      },
      "outputs": [],
      "source": [
        "# make a GET request to the BBC news homepage\n",
        "response = requests.get('https://www.bbc.com/news')\n",
        "\n",
        "# create a BeautifulSoup object from the response content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# find all the main news articles\n",
        "main_articles = soup.find_all('div', class_='gs-c-promo')\n",
        "\n",
        "# iterate through the articles and print their headline, description, and URL\n",
        "for article in main_articles:\n",
        "    try:\n",
        "      headline = article.find('a', class_='gs-c-promo-heading')\n",
        "      description = article.find('p', class_='gs-c-promo-summary')\n",
        "      url = article.find('a', class_='gs-c-promo-heading')['href']\n",
        "      \n",
        "      print('headline:', headline.text.strip())\n",
        "      print('description:',description.text.strip())\n",
        "      print('url:','https://www.bbc.com/'+url)\n",
        "      print()\n",
        "    except:\n",
        "      pass\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xbkJh4LvrQ-5"
      },
      "source": [
        "# Image Scraping\n",
        "\n",
        "![](https://www.enostech.com/wp-content/uploads/2022/04/AdobeStock_474211244.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQfd5stS_DJ9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "os.makedirs('image_scraping_results', exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4NicN1f_d-H"
      },
      "outputs": [],
      "source": [
        "url = 'https://www.webdesignerdepot.com/2009/01/the-evolution-of-apple-design-between-1977-2008/'\n",
        "response = requests.get(url)\n",
        "html_content = response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kf-zvRN_eAc"
      },
      "outputs": [],
      "source": [
        "# soup = BeautifulSoup(html_content, 'lxml')\n",
        "# soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0\"\n",
        "headers = {\"user-agent\": USER_AGENT}\n",
        "\n",
        "resp = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(resp.content, \"lxml\")\n",
        "#print(soup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bE2mA6c_eCi"
      },
      "outputs": [],
      "source": [
        "image_tags = soup.find_all('img')\n",
        "image_urls = [tag['src'] for tag in image_tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOhXynEc_eEp",
        "outputId": "2c9c1831-7092-4fd8-ff55-56d944a1884d"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "for i, url in enumerate(image_urls):\n",
        "    try:\n",
        "      response = requests.get(url)\n",
        "      with open(f'image_scraping_results/image_{i}.jpg', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "        print(f\"-- {c} we found the.jpg format and scrape it\")\n",
        "        c+=1\n",
        "    except:\n",
        "        print(\"!! it is not .jpg format\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r image_scraping_results.zip image_scraping_results >> tmp"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
